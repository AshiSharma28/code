export DATASET_NAME=
export TABLE_NAME=
export TOPIC_NAME=
export JOB_NAME=
export REGION=


gsutil mb gs://$DEVSHELL_PROJECT_ID
bq mk $DATASET_NAME
bq mk --table \
$DEVSHEL_PROJECT_ID:$DATASET_NAME.$TABLE_NAME \
data:string

gcloud pubsub topics create $TOPIC_NAME
gcloud pubsub subscriptions create $TOPIC_NAME-sub --topic=$TOPIC_NAME
gcloud dataflow flex-template run $JOB_NAME --template-file-gcs-location gs://dataflow-templates-$REGION/latest/flex/PubSub_to_BigQuery_Flex --region $REGION --temp-location gs://$DEVSHELL_PROJECT_ID/temp/ --parameters outputTableSpec=$DEVSHELL_PROJECT_ID:$DATASET_NAME.$TABLE_NAME,inputTopic=projects/$DEVSHELL_PROJECT_ID/topics/$TOPIC_NAME,outputDeadletterTable=$DEVSHELL_PROJECT_ID:$DATASET_NAME.$TABLE_NAME,javascriptTextTransformReloadIntervalMinutes=0,useStorageWriteApi=false,useStorageWriteApiAtLeastOnce=false,numStorageWriteApiStreams=0


while true; do
    STATUS=$(gcloud dataflow jobs list --region="$REGION" --format='value(STATE)' | grep Running)
    
    if [ "$STATUS" == "Running" ]; then
        sleep 20
        gcloud pubsub topics publish $TOPIC_NAME --message='{"data": "73.4 F"}'

        bq query --nouse_legacy_sql "SELECT * FROM \`$DEVSHELL_PROJECT_ID.$DATASET_NAME.$TABLE_NAME\`"
        break  # exit the loop since the job is running
    else
        sleep 30 
    fi
done

gcloud pubsub topics publish $TOPIC_NAME --message='{"data": "73.4 F"}'
bq query --nouse_legacy_sql "SELECT * FROM \`$DEVSHELL_PROJECT_ID.$DATASET_NAME.$TABLE_NAME\`"


